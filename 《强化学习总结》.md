# 第0部分
##  第1章 
### 1.6
价值和价值函数是大多数强化学习方法的重要特征。
### 1.7
强化学习的三条主线：试错学习、最优控制、时序差分法

强化学习和有监督学习都使用了误差信息来做一些调整，这是混淆强化学习和有监督学习的

重要因素，但强化学习的整个过程不知道行为是否是正确的。

许多讨论的设计强化学习的方法都是为了讨论如何为每项决策分配功劳。

# 第1部分
## 序章

表格型强化学习问题

赌博机、有限马尔科夫决策过程

动态规划、蒙特卡洛方法、时序差分学习

## 第2章
### 2.1
强化学习和其他机器学习的不同在于，前者的训练信号是用来评估给定动作的好坏的，而不是通过动作范例来直接进行指导。

只有一个状态的简化情况 $\rightarrow$ 非关联性的评估性反馈问题 $\rightarrow$ "k臂赌博机问题"

动作的"$\color{MediumSlateBlue} 价值$"$\rightarrow$ 动作时的"$\color{MediumSlateBlue} 平均期望$"和"$\color{MediumSlateBlue} 收益$"

对于动作a，它的$\textbf{价值}$或者说是$\textbf{收益的期望}$可以表示为：$q_*(a)\doteq\mathbb{E}[R_t|A_t=a]$其中$A_t$表示$t$时刻做出的动作，$R_t$表示$A_t$对应的收益，$a$为动作

然而，实际上，我们并不能准确地知道价值$q_*(a)$，因此，我们将估计$a$在t时刻的价值$Q_t(a)$，使其接近$q_*(a)$。

将估值最高的动作称为"$\color{MediumSlateBlue} \textbf{贪心(greedy)}$"动作，可能存在多个估值最高的动作，从这些动作中选择被称为"$\color{MediumSlateBlue} \textbf{开发(exploitation)}$"关于动作的价值的知识，如果选择非最高的动作则称为 "$\color{MediumSlateBlue} \textbf{试探(exploration)}$"$\rightarrow$总选择最高：开发、选择非最高：探索 $\rightarrow$无法同时开发和探索

确定试探或开发：函数估计、不确定性、剩余时刻的精确数值

无理论假设的情况下会面临：无法保证"最优性(guarantees of
optimality)"和"有界损失性(bounded loss)"

### 2.2 动作-价值方法 (分析动作的价值的方法)
最简单：计算实际收益的平均值来估计动作的"价值"——$\color{MediumSlateBlue} 采样平均方法(sample-average\ method)$:
$\begin{equation}
Q_t(a)\doteq\frac{t时刻前通过执行动作a得到的收益总和}{t时刻前执行动作a的次数}=\frac{\sum_{i=1}^{t-1} R_i\cdot\mathbb{1}_{A_i=a}}{\sum_{i=1}^{t-1} \mathbb{1}_{A_i=a}}\end{equation}$
贪心动作选择$Q_t(a)$最大的动作，是最简单的动作选择方法，记作$A_t\doteq \underset{a}{argmax}Q_t(a)$

上述方法称为$\epsilon-$贪心方法，鲜有人证明有效
### 2.3 $10$臂测试平台 
通过若干次交互积累经验 $\rightarrow$评估性能和动作 $\rightarrow$构成一轮实验

通过若干轮实验$\rightarrow$得到学习算法的平均表现的估计

$\epsilon$ 可以是动态的

价值也会随着时间变化，即非平稳问题

### 2.4 增量式计算

$\begin{equation}Q_{n+1} = \frac{1}{n}\sum_{i=1}^{n}R_i\end{equation}$

可写成

$\begin{equation}Q_n+\frac{1}{n}[R_n-Q_n]\end{equation}$

从而实现怎两式计算而不必将每次的收益都记录下来

更新公式的可以表示为更加一般的形式：

$\begin{equation}新估计值 \leftarrow 旧估计值 + 步长 \times [目标-旧估计值]\end{equation}$

其中 $[目标(target)-旧估计值]$ 是估计值的$\textbf{误差}$，以上面的例子来说，目标就是第n次的收益

步长通常记作$\alpha$或者更加普适地记作$\alpha_t(a)$

### 2.5 跟踪一个非平稳问题

更新规则可以改为:

$\begin{alignat}{2}
Q_{n+1} & \doteq Q_{n}+\alpha[R_n-Q_n], \alpha\in(0,1] \\&
 =(1-a)^n Q_1+\sum_{i=1}^n\alpha(1-\alpha)^{n-i}R_i 
\end{alignat}$

上述过程即根据时间加权平均，具体其可称为"$\color{MediumSlateBlue} 指数近因加权平均(exponential\ recency-weighted\ average)$"

保证序列$\{\alpha_n(a)\}$收敛概率为1地条件为:

$\begin{equation}\sum_{n=1}^\infty\alpha_n(a) =\infty\ and \ \sum_{n=1}^\infty\alpha_n^2(a)< \infty\end{equation}$

这个两个条件不一定满足

练习2.5总结：

平稳问题：

$Q_t(a)$是动作$a$的价值的估计，我们希望$Q_t(a)$能够尽可能地接近真实的价值$q_*(a)$,在构建老虎机bandit时

给每个动作分配一个内存用于记录当前每个动作的价值估计，$\textbf{采样平均方法}$中每个动作的价值之和这个动作相关，不同动作之间的价值评估互相独立

所谓经验在采样平均方法中，由每个动作的价值估计数组表示，数组中值越高表示采用这个动作更好。

非平稳问题：非平稳问题下，采用采样平均和增量式计算的动作-价值方法会导致最优动作的比例下降，而采用常数步长参数的动作-价值可以在一定程度上消除最优动作比例下降的问题。

测试结果如下，红线为$\textbf{采样平均和增量式计算的动作-价值方法}(sample\ averages,\ incrementally\ computed)$的结果，蓝线为$\textbf{常数步长参数的动作-价值方法}(constant\ step-size)$

<p align="center"> <img src='《强化学习总结》图片\练习2_5.png' align="center" > </p> 

### 2.6 乐观初始值
乐观初始值测试结果：

<p align="center"> <img src='《强化学习总结》图片\2_6_乐观初始值测试.png' align="center" > </p> 

但乐观初始值并不能每次都取得较好的效果

练习2.6
是初始值和真实价值之间的差别的大小，具体来说，在计算Qt也就是估计的价值时，采样平均会将初始化值引入价值估计，如果差别较大，在最初，价值估计受初始值影响，与真正的价值区别较大，这促使方法在选择一个动作后趋向于去试探其他动作。如果差别较小，相当于获得了正确的先验信息，能够使方法快速收敛。

练习2.7证明过程：

<p align="center"> <img src='《强化学习总结》图片\练习2_7.jpg' align="center" > </p> 

### 2.7 基于置信度上界的动作选择(Upper-Confidence-Bound Action Selection)
UCB公式
$\begin{equation}At\doteq \underset{a}{argmax}\left[Q_t(a)+c\sqrt{\frac{\ln t}{N_t(a)}}\right]\end{equation}$

附加项用于表示不确定性

UCB尖峰测试
<p align="center"> <img src='《强化学习总结》图片\2_7_UCB尖峰.png' align="center" > </p> 

习题2.8 为什么在11步，因为前十个动作都是满足最大化条件的动作，每个动作都被探索了一下，其中选中高收益后会有效提升平均收益，后续没有找到合适的动作会导致
（chatGPT）：探索与利用权衡：UCB算法在选择动作时，需要权衡探索未知的选项和利用已知的选项之间的平衡。算法通过使用置信上界（confidence bound）来估计每个选项的潜在价值，并选择置信上界最高的选项进行利用。当某个选项的真实价值超过了算法之前估计的置信上界时，UCB算法可能会过度利用该选项，导致性能显著提高。


### 2.8 梯度赌博机算法
偏好函数：

偏好函数的概念是基于$选取动作时，一个动作相对于另一个动作的偏好$，具体来说，通过偏好函数进行动作的选取可以基于如下softmax概率分布进行：

$\begin{equation}Pr\{A_t=a\}\doteq\frac{e^{H_t(a)}}{\sum_{b=1}^ke^{H_t(b)}}\doteq\pi_t(a)\end{equation}$

其中$\pi_t(a)$表示动作$a$在$t$时刻被选取的概率，所有偏好函数的初始值是一样的$H_t(a)=0,\forall a$,也就是说每个动作被选择的概率也是一样的

在课本中，提出了一种自然学习算法，通过如下公式在每个步骤中选择动作$A_t$并获得收益$R_t$后更新偏好函数:

$\begin{align}H_{t+1}(A_t)&\doteq H_t(A_t)+\alpha (R_t-\bar{R_t})(1-\pi_t(A_t)),& and \\ H_{t+1}(a) &\doteq H_t(a) - \alpha(R_t - \bar{R_t}\pi_t(a)),& \forall a \neq A_t    \\ &\doteq H_t(a)+\alpha(R_t-\bar{R_t})(0-\pi _t(a)) \end{align}$

其中$\alpha$为平均步长， $\bar{R_t}$为时刻t内的所有收益的平均值

在10臂平台上与采样平均方法的对比效果：

梯度赌博机算法与采样平均方法的对比（common为采样平均方法）:
<p align="center"> <img src='《强化学习总结》图片\2_8_梯度赌博机算法.png' align="center" > </p> 

在精确的$\textbf{梯度上升算法}$中，$每一个动作的偏好函数H_t(a)$与$增量对性能的影响$成正比

### 2.9 关联搜索(associative search)-抽象概念

$\textbf{关联搜索}$包括$\color{MediumSlateBlue} 试错学习(trial-and-error)$和最优表现和$\color{MediumSlateBlue} 情景(situation)$的关联，介于k臂赌博机和完整的强化学习问题之间，它需要学习策略(policy)。


练习 2.10:
- 如果对情况未知，因为无法区分是动作是A还是B,而$A_1$的期望为$0.1\times \frac{1}{2} + 0.9 \times \frac{1}{2} = 0.5$动作$A_2$的期望为$0.2\times \frac{1}{2} + 0.8 \times \frac{1}{2} = 0.5$，两者相等，都为0.5，因此最优期望是0.5。
- 如果情况已知，则可以根据情况选择最优期望:$0.2\times \frac{1}{2} + 0.9\times\frac{1}{2} = 0.55$

### 2.10 小节
$\color{MediumSlateBlue} epsilon-贪心方法$：在一小段时间内进行随机的动作选择；

$\color{MediumSlateBlue} 基于置信度上界的动作选择(\textbf{UCB})方法$：对具有较小样本的动作进行试探；

$\color{MediumSlateBlue} 梯度赌博机算法$：利用偏好函数而不去估计动作的价值。

## 第2章 有限马尔科夫决策过程(Finite Markov Decision Processes)
### 介绍
赌博机的价值为$q_*(a)$，MDP的价值为$q_*(s,a)$表示动作a在每个状态s中的价值,MDP的价值也可以是$v_*(s)$,表示给定最优动作下的每个状态的价值。

$q_*(s,a)$和$v_*(s)$的区别是啥：

chatGPT:
>在马尔可夫决策过程（MDP）中，价值函数是对每个状态或状态-动作对的预期收益进行评估和比较的一种方法。在MDP中，有两种常见的价值函数形式，分别是动作值函数（action-value function） $q_*(s,a)$和状态值函数（state-value function）$v_*(s)$
价值
>- 动作值函数 $q_*(s,a)$：动作值函数衡量在给定状态$s$下，采取动作$a$所能获得的预期累计回报。它表示在状态$s$选择动作$a$时，经过一系列的决策和行动后，得到的累积回报的期望值。
>- 状态价值函数$v_*(s)$：状态值函数衡量在给定状态 
$s$下，从该状态开始采取最优策略所能获得的预期累计回报。它表示经过一系列的决策和行动后，从状态 $s$开始，按照最优策略进行行动所得到的累积回报的期望值。
>
>简而言之，动作值函数$q_*(s,a)$是在给定状态和动作下的预期回报，而状态值函数 $v_*(s)$是在给定状态下按照最优策略所能获得的预期累计回报。

这两种价值函数都在强化学习中起到重要的作用，它们帮助智能体评估不同状态和动作的质量，并指导智能体做出最优的决策和行动。

总结：$q_*(s,a)$是每个动作中每个状态的累计价值，而$v_*(s)$为每个动作下所有最优动作的累计价值

### 3.1 “智能体-环境”交互接口
####  $\color{yellow}定义：$

智能体(agent)：输入状态(state)和收益(reword)，输出动作(action)

环境(environment)：输入动作，输出新状态(state)和收益(reward)

$\color{MediumSlateBlue} 动作(action)$定义智能体做出的选择

$\color{MediumSlateBlue} 状态(state)$定义智能体做出的选择的基础

$\color{MediumSlateBlue} 收益(reward)$定义智能体的目标

如图

<p align="center">
  <img src='《强化学习总结》图片\3_1_智能体-环境交互.png' alt='智能体与环境交互示意图' align="center" >
</p>

 MDP和智能体共同给出了一个序列或者轨迹：
 $S_0,A_0,R_1,S_1,A_1,R_2,S_3...$

$R_t$和$S_t$只依赖于前继状态和动作。
在$有限$MDO中，状态、动作和收益的集合为S、A和R，这种情况下，随机变量$R_t$和$S_t$具有定义明确的概率分布，它的形式化定义为：

$\begin{equation} p(s',r|s,a)\doteq Pr\{S_t=s',R_t=r|S_{t-1} = s,A_{t-1}=a\} \end{equation}$

其中，p是确定性函数
$\begin{equation} p:S\times R \times S\rightarrow [0,1] \end{equation}$

$\begin{equation} \sum_{s'\in S} \sum_{r \in R} p(s',r|s,a) = 1, 对于所有s \in S,a \in A(s)  \end{equation}$

p刻画了$环境(Environment)$的动态特性(注：动态特性指系统运行时输入量和输出量之间的关系)：

针对状态，$S_t$和$R_t$只和$S_{t-1}$和$A_{t-1}$有关，因此状态具有马尔可夫性

#### $\color{yellow} 马尔可夫性：$

wiki对马尔可夫性的解释：

>wiki：在概率论和统计学中，马尔可夫性质（Markov property）指的是随机过程的无记忆性质，也就是说其未来的演化与其历史无关。如果一个随机过程的未来状态的条件概率分布（在过去和现在的值的条件下）仅取决于当前状态，那么该随机过程具有马尔可夫性质；需要注意的是，在英文定义的简明陈述中经常被忽视的一个微妙但非常重要的点。即过程的状态空间在时间上保持不变。条件描述涉及一个固定的"带宽"。例如，如果没有这个限制，我们可以将任何过程扩展为包括从给定初始条件开始的完整历史$（\color{yellow} 将从初始到当前的完整过程视为一个步骤?）$，从而使其成为马尔可夫过程。但状态空间在时间上会呈递增的维度，并不符合定义。

这与书本P47最上方一段的内容“状态必须包括过去智能体和环境交互的方方面面，这些信息对未来产生一定的影响”看似矛盾，实际上隐含了一个意思，就是：“过去的经历起始已经融入了当前的状态，现在的位置是过去的经历决定的”
[马尔可夫性的解释](https://towardsdatascience.com/markov-chains-a-take-on-life-35614859c99c)

$p(s',r|s,a)$可以引申出其他信息，比如：$状态转移概率p(s'|s,a)$、“状态-动作”二元组期望收益$r(s,a)$和“状态-动作-后继状态”三元组的期望收益r(s,a,s')，其中三元组的期望收益表示在发生某种状态转义情况下的期望收益。

#### $\color{yellow} 智能体和环境的边界：$

智能体(agent)和环境的边界，并非物理边界，对于人来说，肌肉、骨骼和感知器官属于环境(environment)。

智能体无法改变的事物（智能体可能知道但无法改变）$\rightarrow$环境的一部分（外部）

收益由无法改变的目标而来$\rightarrow$外部

智能体可能知道外部的一切，但无法很好地解决问题，如“魔方”

$\color{MediumSlateBlue} “边界(boundary)”$就是智能体绝对控制的边界，而不是知识的边界。

#### $\color{yellow} 转移图（Trainsition\ Graph）：$

状态节点:$(s)$

动作节点:$(s,a)$

转移箭头:$(s,s',a)$,箭头上标有转移概率$p(s'|s,a)$和期望收益$r(s,a,s')$

<p align="center">
  <img src='《强化学习总结》图片\3_1_转移图.png ' alt='转移图示例——捡易拉罐机器人' align="center" >
</p>

#### $\color{yellow} 练习3.4：$

| $s$ | $a$ | $s'$ | $r$ | $p(s',r\|s,a)$ |
| -------- | -------- | -------- | -------- | -------- |
| $high$ | $search$ | $high$ |  $r_{research}$ | $\alpha$ |
| $high$ | $search$ | $high$ | $r_{wait}$ | $0$ |
| $high$ | $search$ | $high$ | $-3$ | $0$ |
| $high$ | $search$ | $high$ | $0$ | $0$ |
| $high$ | $search$ | $low$ |  $r_{research}$ | $1-\alpha$ |
| $high$ | $search$ | $low$ | $r_{wait}$ | $0$ |
| $high$ | $search$ | $low$ | $-3$ | $0$ |
| $high$ | $search$ | $low$ | $0$ | $0$ |
| $low$ | $search$ | $high$ |  $r_{research}$ | $0$ |
| $low$ | $search$ | $high$ | $r_{wait}$ | $0$ |
| $low$ | $search$ | $high$ | $-3$ | $1-\beta$ |
| $low$ | $search$ | $high$ | $0$ | $0$ |
| $low$ | $search$ | $low$ |  $r_{research}$ | $\beta$ |
| $low$ | $search$ | $low$ | $r_{wait}$ | $0$ |
| $low$ | $search$ | $low$ | $-3$ | $0$ |
| $low$ | $search$ | $low$ | $0$ | $0$ |
| $high$ | $wait$ | $high$ |  $r_{research}$ | $0$ |
| $high$ | $wait$ | $high$ | $r_{wait}$ | $1$ |
| $high$ | $wait$ | $high$ | $-3$ | $0$ |
| $high$ | $wait$ | $high$ | $0$ | $0$ |
| $high$ | $wait$ | $low$ |  $r_{research}$ | $0$ |
| $high$ | $wait$ | $low$ | $r_{wait}$ | $0$ |
| $high$ | $wait$ | $low$ | $-3$ | $0$ |
| $high$ | $wait$ | $low$ | $0$ | $0$ |
| $low$ | $wait$ | $high$ |  $r_{research}$ | $0$ |
| $low$ | $wait$ | $high$ | $r_{wait}$ | $0$ |
| $low$ | $wait$ | $high$ | $-3$ | $0$ |
| $low$ | $wait$ | $high$ | $0$ | $0$ |
| $low$ | $wait$ | $low$ |  $r_{research}$ | $0$ |
| $low$ | $wait$ | $low$ | $r_{wait}$ | $1$ |
| $low$ | $wait$ | $low$ | $-3$ | $0$ |
| $low$ | $wait$ | $low$ | $0$ | $0$ |
| $low$ | $recharge$ | $high$ |  $r_{research}$ | $0$ |
| $low$ | $recharge$ | $high$ | $r_{wait}$ | $0$ |
| $low$ | $recharge$ | $high$ | $-3$ | $0$ |
| $low$ | $recharge$ | $high$ | $0$ | $1$ |
| $low$ | $recharge$ | $low$ |  $r_{research}$ | $0$ |
| $low$ | $recharge$ | $low$ | $r_{wait}$ | $0$ |
| $low$ | $recharge$ | $low$ | $-3$ | $0$ |
| $low$ | $recharge$ | $low$ | $0$ | $0$ |

### 3.2 目标(Goal)和收益(Reword)

目标(Goal)可以归结为：最大化智能体接收到的标量信号（称之为收益(reward)）的$\color{yellow} 累计和$的$\color{yellow} 概率期望值$

强化学习用收益信号来形式化目标

收益信号只传达什么是你想要实现的目标，而不是如何实现这些目标。因此收益信号并不传授智能体如何实现目标的先验知识。


### 3.3 回报(Returns)和分幕(Episodes)


#### $\color{yellow} 回报：$
目标的正式定义：寻求最大化$\color{yellow} “期望回报”$，记为$\color{yellow} G_t$,它被定义为收益序列的一些特定函数，例如
$\begin{equation} G_t\doteq R_{t+1}+R_{t+2}+R_{t+3}+...+R_T \end{equation}$
其中，$T$表示终结时刻

#### $\color{yellow} 分幕任务：$
在有“最终时刻”这种概念的应用中，智能体和环境的交互被自然地分成一系列子序列，每个子序列称为$\color{yellow} 幕(episodes)$，如一盘游戏，一次迷宫，每一幕都以特殊状态结束，称为$\color{yellow} 终结状态(terminal state)$。

非终结状态集，记为$\color{yellow}S$，包含终结状态的集合为$\color{yellow} S^+$，终结时刻$\color{yellow} T$是一个随机变量

#### $\color{yellow} 持续性任务：$
无法被分为单独的幕、控制过程连续、长期运行，$T=\infty$，回报趋于无穷。

#### $\color{yellow} 折扣$：
折扣是系数，折后回报如下：
$\begin{equation}G_t\doteq R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+ ... = \sum_{k=0}^\infty \gamma^k R_{t+k+1}\end{equation}$
如果$\gamma<1$，则当$R_{k}$有界，则$G_t$为一个有限值，这根据系数为$R_{k}$的上界，比例为$\gamma$的等比数列求和公式的极限可知，例如，假设$R_{k}$的上界为1，则
$\begin{equation} G_t=\sum_{k=0}^\infty \gamma^k = \frac{1}{1-\gamma}\end{equation}$

邻接关系为：
$\begin{equation}G_t\doteq R_{t+1}+\gamma G_{t+1}\end{equation}$

练习 3.5 
$\begin{equation} \sum_{s'\in S^+}\sum_{r \in R}p(s',r|s,a)=1 ，对于所有s \in S^+,a\in A(s) \end{equation}$

练习 3.6
> 回报正比于$-\gamma^k,k < T$，除了多了终止状态$T$外，区别不大。
>[参考答案](https://www.cnblogs.com/initial-h/p/14575808.html)

练习 3.7 
>由于没有折扣系数，因此通关快慢没有影响收益，也可能是由于收益0太多，收益过于稀疏导致。[参考答案](https://www.cnblogs.com/initial-h/p/14575808.html)

练习 3.8

联立方程递归求解:

$\begin{cases}R_5=2\\
G_{5}=R_5\\
G_{t}=R_{t+1}+\gamma G_{t+1}\end{cases}$

练习 3.9

$G_1=7\sum_{k=1}^\infty \gamma^k=\frac{7}{1-\gamma} = 70\\
G_0=R_1+\gamma G_1=65$

练习 3.10

等比数列求和后取极限:
$\begin{equation} \lim_{n\to\infty} \frac{1-\gamma^n}{1-\gamma}=\frac{1}{1-\gamma}\end{equation}$

### 3.4 分幕式和持续性任务的统一表达

将回报表示为
$\begin{equation}G_t\doteq\sum_{k=t+1}^{T} \gamma^{k-t-1}R_k \end{equation}$
将终结状态之后的步骤视为无收益的无限循环。

### 3.5 策略和价值函数

#### $\color{yellow} 价值函数$：
价值函数用来评估智能体(agent)在给定状态(或给定状态与动作)下有多好，也就是回报的期望的高低。

#### $\color{yellow} 策略$：
简单来说，策略是由价值函数采取的特定的行为方式\
严格来说，策略是从状态到每个动作的选择概率之间的映射，表示为
$\begin{equation} \pi(a|s) \end{equation}$
表示，在$t$时刻，当$S_t=s$时$A_t=a$的概率\
强化学习方法规定了智能体的策略如何随着其经验而发生变化

练习 3.11 

我的答案：
$\begin{equation} \sum_{r\in R} r \sum_{s' \in S}  \sum_{a \in A(s)} \pi(a|s)p(s',r|s,a)\end{equation}$

> $\begin{equation} \mathbb{E}[R_{t+1}|S_{t+1}]=\sum_{s_{t+1}\in S}\sum_{r_{t+1}\in R}\sum_{a \in A}r_{t+1}p(s_{t+1},r_{t+1}|s_t,a_t)\pi(a_t|s_t) \end{equation}$[参考答案](https://www.cnblogs.com/initial-h/p/14575808.html)

#### $\color{yellow} 价值函数的形式化定义$：

$\begin{equation}v_{\pi}(s)\doteq\mathbb{E}_{\pi}[G_t|S_t=s]=\mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty} \gamma^kR_{t+k+1} \middle| S_t=s\right]\end{equation}$
称$v_{\pi}$为策略$\pi$的状态价值函数(与状态相关)，是策略所得的回报的概率期望值

$\begin{equation} q_{\pi}(s,a)\doteq\mathbb{E}_{\pi}\left[ G_t \middle|S_t=s,A_t=a \right]=\mathbb{E}_{\pi}\left[ \sum_{k=0}^\infty \gamma^kR_{t+k+1} \middle| S_t=t,A_t=a \right] \end{equation}$
称$q_{\pi}$为策略$\pi$的动作价值函数(与状态和动作相关)，是从状态s开始，执行动作$a$后，所有可能的决策序列的期望回报

练习3.12

$\begin{equation}v_{\pi}(s)=\sum_{a\in A_t}q_{\pi}(s,a)\pi(a|s)\end{equation}$

练习3.13

>参考书本公式3.14(贝尔曼方程)和练习3.12得
>$\begin{equation} q(s,a)=\sum_{s',a}p(s',r|s,a)[r+\gamma v_{\pi}(s')] \end{equation}$

#### $\color{yellow} 贝尔曼方程$：
策略$\pi$和任何状态$s$,$s$的价值与后继状态存在一下关系：
$\begin{equation}v_{\pi}(s)\doteq\sum_{a}\pi(a|s)\sum_{s',r}p(s',r|s,a)\left[r+\gamma v_{\pi}(s') \right]\end{equation}$
贝尔曼方程对所有可能性采用其出现概率进行了加权平均

#### $\color{yellow} 回溯图$
<p align="center">
  <img src='《强化学习总结》图片\回溯图.png ' alt='回溯图' align="center" >
</p>
空心圆为一个状态，实心圆为一个“状态-动作”二元组、
回溯操作就是将后继状态的价值信息回传给当前时刻的状态(或“状态-动作”二元组)。

练习 3.14 

$\begin{equation} \frac{0.9(2.3+0.4-0.4+0.7)}{4} \end{equation}=\frac{2.7}{4}\approx 0.7$

练习 3.15

>$\begin{align} \hat{v}_{\pi}(s) &= \mathbb{E}_{\pi}(\hat{G}_t|S_t=s)\\
&=\mathbb{E}_{\pi}\left[\sum_{k=0}^\infty \gamma^k \cdot \hat{R}_{t+k+1} + c \middle| S_t=s \right]\\
&=\mathbb{E}_{\pi}\left[\sum_{k=0}^\infty \gamma^k \cdot \hat{R}_{t+k+1} \middle|S_t=s \right] + \mathbb{E}_{\pi}\left[\sum_{k=0}^\infty \gamma^k\cdot \middle| S_t=s \right] \\ 
&=\mathbb{E}_{\pi}\left[ \sum_{k=0}^\infty \gamma^k \cdot R_{t+k+1} \middle| S_t=s \right] + \sum_{k=0}^\infty \gamma^k \cdot c \\
&=v_{\pi}(s) + \frac{c}{1-\gamma}\end{align}$
>
>综上
>$\begin{equation}v_{c} = \frac{c}{1-\gamma}\end{equation}$
>[参考答案2](https://blog.csdn.net/ballade2012/article/details/90581230)

练习 3.16 

我的答案：不影响（错误）

正确答案：
>$\begin{align} \hat{v}_{\pi}(s) &= \mathbb{E}_{\pi}(\hat{G}_t|S_t=s)\\
&=\mathbb{E}_{\pi}\left[\sum_{k=0}^K \gamma^k \cdot \hat{R}_{t+k+1} + c \middle| S_t=s \right]\\
&=\mathbb{E}_{\pi}\left[\sum_{k=0}^K \gamma^k \cdot \hat{R}_{t+k+1} \middle|S_t=s \right] + \mathbb{E}_{\pi}\left[\sum_{k=0}^K \gamma^k\cdot \middle| S_t=s \right] \\ 
&=\mathbb{E}_{\pi}\left[ \sum_{k=0}^K \gamma^k \cdot R_{t+k+1} \middle| S_t=s \right] + \sum_{k=0}^K\gamma^k \cdot c \\
&=v_{\pi}(s) + \frac{c(1-\gamma^K)}{1-\gamma}\end{align}$
>$\frac{c(1-\gamma^K)}{1-\gamma}$与步数$K$相关，因此有影响

练习 3.17

结合贝尔曼方程和练习3.12 : $v_{\pi}(s')=\sum_{a'\in A_t}q_{\pi}(s',a')\pi(a'|s')$ 

$\begin{align}q_{\pi}(s,a)&=\sum_{s',r} p(s',r|s,a)[r+\gamma v_{\pi}(s')]\\
&=\sum_{s',r} p(s',r|s,a)[r+\gamma \sum_{a'}q_{\pi}(s',a')\pi(a'|s')]\end{align}$

练习 3.18

>$\begin{align}v_{\pi}(s)&=\mathbb{E}_{\pi}[q_{\pi}(s,a_1)+q_{\pi}(s,a_2)+q_{\pi}(s,a_3)] \\&=\pi(a_1|s)q_{\pi}(s,a_1)+\pi(a_2|s)q_{\pi}(s,a_2)+\pi(a_3|s)q_{\pi}(s,a_3)\end{align}$

练习 3.19

>$\begin{align} q_{s,a}&=\mathbb{E}_{\pi}[r_1+\gamma v_{\pi}(s_1')] \\&+ \mathbb{E}_{\pi}[r_2+\gamma v_{\pi}(s_2')] \\&+ \mathbb{E}_{\pi}[r_3+\gamma v_{\pi}(s_3')] \\&=p(s_1',r_1|s,a)[r_1+\gamma v_{\pi}(s_1')] \\&+ p(s_2',r_2|s,a)[r_2+\gamma v_{\pi}(s_2')]\\&+ p(s_3',r_3|s,a)[r_3+\gamma v_{\pi}(s_3')]\end{align}$

### 3.6 最优策略和最优价值函数

#### $\color{yellow} 最优策略$：
对于所有$s\in S$，当且仅当$v_{\pi}(s)\ge v_{\pi'}(s)$时，$\pi \ge \pi'$。那么，
总会有一个策略不劣于其他策略，这就是$\color{yellow} "最优策略(optimal\ policies)"$。

用$\pi_*$表示$最优策略$，

用$v_*$表示$最优状态价值函数$
定义为：
对于任意的$s \in S$
$\begin{equation} v_*(s)\doteq \underset{\pi}{max}\ v_{\pi}(s)  \end{equation}$

用$q_*$表示$最优动作价值函数$
定义为：
对于任意的$s \in S,\ a\in A$
$\begin{equation} q_*(s,a)\doteq \underset{\pi}{max}\ v_{\pi}(s,a)  \end{equation}$
$q_*$表示，在采取动作$a$后，采用最优策略得到的期望回报，因此$q_*(s,a)$也可以表示为:
$\begin{equation} q_*(s,a) = \mathbb{E}[R_{t+1}+\gamma v_*(S_{t+1})|S_t=s,A_t=a] \end{equation}$

#### $\color{yellow} 贝尔曼最优方程$：
$\begin{align} v_*(s) &= \mathbb{E}[ R_{t+1}+\gamma G_{t+1}|S_t=s,A_t=a]\\&= \underset{a}{max}\sum_{s',r}p(s',r|s,a)[r+\gamma v_*(s')] \end{align}$

$\begin{align} q_(s,a)&=\mathbb{E}[R_{t+1}+\gamma \underset{a'}{max}q_*(S_{t+1},a')|S_t=s,A_t=a]\\ &=\sum_{s',r}p(s',r|s,a)[r+\gamma \underset{a}{max}q_*(s',a')]\end{align}$

v*回溯图:
<p align="center">
  <img src='《强化学习总结》图片\v_star.png' alt='v*回溯图' align="center" >
</p>
q*回溯图:
<p align="center">
  <img src='《强化学习总结》图片\q_star.png' alt='q*回溯图' align="center" >
</p>

#### $\color{yellow} 最优方程的作用$：
- 对于最优价值函数$v_*$来说，单步搜索后的最好动作就是全局最优动作，任何$贪心$策略都是最优策略。\
$v_*$本身已经包含了未来所有可能的行为所产生的回报的影响。

- 对于最优动作函数$q_*$，不需要进行单步搜索的过程，只要得到使$q_*(s,a)$最大化的动作$a$就行。


练习3.21

| $state$ | $q_*(s,putter)$ | 
| -------- | -------- |
| $-6$ | $-4$ |
| $-5$ | $-3$ |
| $-4$ | $-3$ |
| $-3$ | $-2$ |
| $-2$ | $-2$ |
| $-1$ | $-1$ |

练习3.22

$v_{\pi_{left}} = 1+0*\gamma+1*\gamma^2+0*\gamma^3+ ... = \sum_{k=0}^\infty \gamma^{2k}$

$v_{\pi_{right}} = 0+2*\gamma+0*\gamma^2+2*\gamma^3 +... = \sum_{k=0}^{\infty} \gamma^{2k+1}$

$当\gamma=0 \rightarrow v_{\pi_{left}}=1>v_{\pi_{right}}=0 $
$当\gamma=0.9 \rightarrow v_{\pi_{left}}\approx 5.26<v_{\pi_{right}}\approx 9.47 $
$当\gamma=0.5 \rightarrow v_{\pi_{left}}\approx 1.33 = v_{\pi_{right}}\approx 1.33 $

练习 3.23

$q_*(h,s)=\alpha[r_s+\gamma\ \underset{a'}{max}\ q_*(h,a')]+(1-\alpha)[r_s+\gamma\ \underset{a'}{max}\ q_*(l,a')]$

$q_*(l,s)=(1-\beta)[-3+\gamma\ \underset{a'}{max}\ q_*(h,a')]+\beta[r_s+\gamma\ \underset{a'}{max}\ q_*(l,a')]$

$q_*(h,w)=1\times[r_w+\gamma\ \underset{a'}{max}\ q_*(h,a')]$

$q_*(l,w)=1\times[r_w+\gamma\ \underset{a'}{max}\ q_*(l,a')]$

$q_*(l,re)=\gamma\ \underset{a'}{max}\ q_*(l,a')$

练习 3.24 

$\begin{align} G_t&\doteq R_{t+1}+\gamma R_{t+2} + \gamma^2 R_{t+3} + ... \&=\frac{10}{1-\gamma^5} \end{align}$
取$\gamma = 0.9$
$\begin{equation} G_t = 24.419 \end{equation}$

练习 3.25

$v_*(s)=\underset{a}{max}\ q_*(s,a)$

练习 3.26 

参考公式3.19和3.20:
$q_*=\sum_{s',r}p(s'.r|s,a)[r+\gamma v_*(s')]$

练习 3.27 
 > $\pi_*(a|s)=\begin{cases} 1,& \text{if}\ a = \underset{a\in A(s)}{argmax}(q_*(a,s)) \\
 0,& \text{else} \end{cases}$

 练习 3.28 
 > $\pi_*(a|s)=\begin{cases} 1,& \text{if}\ a = \underset{a\in A(s)}{argmax}(\sum_{s',r}p(s'.r|s,a)[r+\gamma v_*(s')]) \\
 0,& \text{else} \end{cases}$

 练习 3.29
 $\begin{equation} v_\pi=\sum_a \pi(a|s)\sum_{s'}p(s'|s,a)[r+\gamma v_{\pi}(s')] \end{equation}$
 
 $\begin{equation} v_\pi=\sum_a \pi(a|s)r(s,a)[r+\gamma v_{\pi}(s')] \end{equation}$
 
 $\begin{equation} v_*=\underset{a}{max}\sum_{s'}p(s'|s,a)[r+\gamma v_{*}(s')] \end{equation}$

 $\begin{equation} v_*=\underset{a}{max}\ r(s,a)[r+\gamma v_{*}(s')] \end{equation}$
 
 $\begin{equation} q_\pi=\sum_{s'}p(s'|s,a)[r+\gamma v_{\pi}(s')] \end{equation}$
 
 $\begin{equation} q_\pi=r(s,a)[r+\gamma v_{\pi}(s')] \end{equation}$
 
 $\begin{equation} q_*=\sum_{s'}p(s'|s,a)[r+\gamma \underset{a'}{max}\ q_{*}(s',a')] \end{equation}$

 $\begin{equation} q_*=r(s,a)[r+\gamma \underset{a'}{max}\ q_{*}(s',a')] \end{equation}$


## 第4章 动态规划(Dynamic Programming)
### 介绍
DP (Dynamic Programming)在给定一个用马尔可夫决策过程描述的完备环境模型的情况下，其可以计算最优的策略。

### 4.1 策略评估-预测(Policy Evaluation - Prediction)

$\color{yellow}期望更新$：\
近似使用$v_\pi$的贝尔曼方程：
$\begin{align}v_{k+1}&\doteq\mathbb{E}\left[ R_{t+1} + \gamma v_{k}(S_{t+1})\middle | S_t=s \right]\\&=\sum_a \pi\left(a \middle | s\right)\sum_{s',r} p\left(s',r\middle | s,a\right)\left[r+\gamma v_k(s') \right]\end{align}$

更新规则为，将单步转移之后的即时收益$R_{t+1}$与s的每个后继状态的旧的价值函数$v_k(s')$的和的期望来更新新的价值函数

疑问:$v_{k}$和$v_{k+1}$的区别是什么呢

$\color{yellow} 不动点(Fixed Point)$：\
迭代过程中f(x)=x的点为不动点，即函数映射到其自身的一个点。

$\color{yellow}就地更新$：\
用计算出来的$v_{t+1}$替换旧的价值函数$v_{k}$。

$\color{yellow}根据命题进行程序设计$
案例，迭代求解例3.5中的期望函数，见3_5_example.py。\
步骤1：构建状态空间S、构建收益空间R、构建动作空间A\
步骤2：构建状态转移方程\
步骤3：构建$p(s',r|s,a)$\
步骤4：构建$q(s,a)$\
步骤5：构建$v(s)$\
步骤6：构建策略评估算法 (中文课本P73)

练习 4.1:\
$q_\pi(11,down)= \underset{s_{end},r}{\mathbb{E}}\left[r+\gamma v_\pi(s_{end})\right]=\sum_{s_{end},a}p(s_{end},r|s,a)[r+\gamma v_{\pi}(s_{end})]=1\times(-1+0)= -1 $

$q_\pi(7,down)= \underset{s_{11},r}{\mathbb{E}}\left[r+\gamma v_\pi(s_{11})\right]=\sum_{s_{11},a}p(s_{11},r|s,a)[r+\gamma v_{\pi}(s_{11})]=1\times(-1-14)= -15 $

练习 4.2:\
$\begin{align}v_\pi(15) &= \frac{-1+v_\pi{12}}{4}+\frac{-1+v_\pi{13}}{4}+\frac{-1+v_\pi{14}}{4}+\frac{-1+v_\pi{15}}{4} \\&=\frac{-1+-22}{4}+\frac{-1-20}{4}+\frac{-1-14}{4}+\frac{-1+v_\pi{15}}{4}\\&=-20\end{align}$

>列为二元一次方程组:
$\begin{equation}\begin{cases}v_\pi{15} = \frac{1}{4}(-1-22)+\frac{1}{4}(-1+v_\pi{(13)})+\frac{1}{4}(-1-14)+\frac{1}{4}(-1+v_\pi{(15)})\\v_\pi{13} = \frac{1}{4}(-1-22)+\frac{1}{4}(-1-20)+\frac{1}{4}(-1-14)+\frac{1}{4}(-1+v_\pi{(15)})\end{cases}\end{equation}$

疑问：其他价值不需要改变吗

练习 4.3：
$\begin{align}q_{\pi}(s,a)&=\mathbb{E}(R_{t+1}+\gamma G_{t+1}|S_{t}=s,A_{t}=a)\\
&=\sum_{s',r} p(s',r|s,a)[r+\gamma \sum_{a'}q_{\pi}(s',a')\pi(a'|s')]\end{align}$

$\begin{align}q_{t+1}(s,a)=\sum_{s',r} p(s',r|s,a)[r+\gamma \sum_{a'}q_{t}(s',a')\pi(a'|s')]\end{align}$

### 4.2 策略改进(policy improvement)

$\color{yellow} 策略改进定理$：\
特例：找到一个不同于原有策略的动作a，使得在每次遇到状态s的时候，选择动作a可以达到更好的效果。即：\
$\begin{align} q_{\pi}(s,\pi'(s))\ge v_{\pi}(s) \end{align}$
$\begin{align} v_{\pi'}(s)\ge v_{\pi}(s) \end{align}$
称策略$\pi'$由于策略$\pi$

根据原策略函数执行贪心算法，来构造一个更好策略的过程，称为$\color{yellow}策略改进(policy\ improvement)$\
策略改进一定会给出一个更优的结果。




